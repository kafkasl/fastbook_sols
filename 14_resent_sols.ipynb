{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questionnaire"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__1. How did we get to a single vector of activations in the CNNs used for MNIST in previous chapters? Why isn't that suitable for Imagenette?__\n",
    "\n",
    "We used enough stride 2 convs to ensure that the final layer has a activation map dimension of 1x1. Then we used nn.Flatten() to remove the 1x1 unit axes. This gives us a matrix of activations by batch. This method in MNIST assumed a starting image size of 28x28. \n",
    "\n",
    "This method is not suiable for Imagette because our images might be larger than 28x28.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__2. What do we do for Imagenette instead?__\n",
    "\n",
    "We use fully convolution neural networks. In these networks we take the average of activations across the convolutional grid. This will convert a grid of activations into a single activation per image.\n",
    "\n",
    "TODO: Does a FC network require a linear layer at the end?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__3. What is \"adaptive pooling\"?__\n",
    "\n",
    "In average pooling you set the ks and the stride yourself. \n",
    "\n",
    "In adaptive average pooling you specify the output. The ks and stride are auto detected. It's less work for you.\n",
    "\n",
    "https://stackoverflow.com/questions/58692476/what-is-adaptive-average-pooling-and-how-does-it-work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__4. What is \"average pooling\"?__\n",
    "\n",
    "See above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__5. Why do we need `Flatten` after an adaptive average pooling layer?__\n",
    "\n",
    "The activation map has a unit dimension (1x1) after the adaptive average pooling layer. `nn.Flatten()` removes the unit axes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__6. What is a \"skip connection\"?__\n",
    "\n",
    "In a skip connection we do `x+conv2(conv1(x))`. The batchnorm has a gamma set to 0. As a result `x+conv2(conv1(x))` will be equal to `x` initially. So the network does nothing and returns the identity matrix.\n",
    "\n",
    "The parameters are then trained to make the skip connection more useful. (Kind of like a call option - the downside is limited to the original matrix in the upside is unlimited?)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__7. Why do skip connections allow us to train deeper models?__\n",
    "\n",
    "A resent is good at measuring Opportunity Cost. \n",
    "\n",
    "It measures the cost of doing nothing (returning the identity) vs the cost of doing something (passing the block through two conv layers with trainable weights). \n",
    "\n",
    "Sometimes doing nothing is a valid choice. In this case the network places less emphasis on the resblock - saving time and valuable compute.\n",
    "\n",
    "This allows us to train deeper models.\n",
    "\n",
    "TODO: check if this is right?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__8. What does <<resnet_depth>> show? How did that lead to the idea of skip connections?__\n",
    "\n",
    "The resnet depth graphs shows that more layers does not necessarily yield better results. \n",
    "\n",
    "The 56 layer model has a worse training and testing error compared to the 20 layer model.\n",
    "\n",
    "This led to the idea of starting with a 20 layer network and addding 36 additional layers that do nothing. The 56 layer network should be at least as good as the 20 layer network. \n",
    "\n",
    "But the additional 36 layers have trainable parameters. So there's an (almost) asymmetric risk/reward.\n",
    "\n",
    "The worst that can happen is that the loss is the same as the 20 layer network. The best thing that can happen is that the loss is significantly better than the 20 layer network.\n",
    "\n",
    "The downside  is limited to the original loss of 20 layer network. The upside is unlimited - the loss can be as low as possible. (TODO: maybe rephrase this?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__9. What is \"identity mapping\"?__\n",
    "\n",
    "A mapping that returns the original input matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__10. What is the basic equation for a ResNet block (ignoring batchnorm and ReLU layers)?__\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "1. What do ResNets have to do with residuals?\n",
    "1. How do we deal with the skip connection when there is a stride-2 convolution? How about when the number of filters changes?\n",
    "1. How can we express a 1Ã—1 convolution in terms of a vector dot product?\n",
    "1. Create a `1x1 convolution` with `F.conv2d` or `nn.Conv2d` and apply it to an image. What happens to the `shape` of the image?\n",
    "1. What does the `noop` function return?\n",
    "1. Explain what is shown in <<resnet_surface>>.\n",
    "1. When is top-5 accuracy a better metric than top-1 accuracy?\n",
    "1. What is the \"stem\" of a CNN?\n",
    "1. Why do we use plain convolutions in the CNN stem, instead of ResNet blocks?\n",
    "1. How does a bottleneck block differ from a plain ResNet block?\n",
    "1. Why is a bottleneck block faster?\n",
    "1. How do fully convolutional nets (and nets with adaptive pooling in general) allow for progressive resizing?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
