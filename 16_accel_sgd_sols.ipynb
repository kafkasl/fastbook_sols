{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questionnaire"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__1. What is the equation for a step of SGD, in math or code (as you prefer)?__\n",
    "\n",
    "\n",
    "`p.data.add_(-lr, p.grad.data)`\n",
    "\n",
    "params = params - lr * params.gradient\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__2. What do we pass to `cnn_learner` to use a non-default optimizer?__\n",
    "\n",
    "You pass the following:\n",
    "`opt_func=partial(Optimizer, cbs=[sgd_cb])`\n",
    "\n",
    "where `def sgd_cb(p, lr, **kwargs): p.data.add_(-lr, p.grad.data)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__3. What are optimizer callbacks?__\n",
    "\n",
    "This: `opt_func = partial(Optimizer, cbs=[sgd_cb])`\n",
    "\n",
    "They help us run a desired optimiser (eg. sgd) using a few lines of code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__4. What does `zero_grad` do in an optimizer?__\n",
    "\n",
    "- Loops through params and sets gradients to 0\n",
    "- Removes gradient history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__5. What does `step` do in an optimizer? How is it implemented in the general optimizer?__\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "1. Rewrite `sgd_cb` to use the `+=` operator, instead of `add_`.\n",
    "1. What is \"momentum\"? Write out the equation.\n",
    "1. What's a physical analogy for momentum? How does it apply in our model training settings?\n",
    "1. What does a bigger value for momentum do to the gradients?\n",
    "1. What are the default values of momentum for 1cycle training?\n",
    "1. What is RMSProp? Write out the equation.\n",
    "1. What do the squared values of the gradients indicate?\n",
    "1. How does Adam differ from momentum and RMSProp?\n",
    "1. Write out the equation for Adam.\n",
    "1. Calculate the values of `unbias_avg` and `w.avg` for a few batches of dummy values.\n",
    "1. What's the impact of having a high `eps` in Adam?\n",
    "1. Read through the optimizer notebook in fastai's repo, and execute it.\n",
    "1. In what situations do dynamic learning rate methods like Adam change the behavior of weight decay?\n",
    "1. What are the four steps of a training loop?\n",
    "1. Why is using callbacks better than writing a new training loop for each tweak you want to add?\n",
    "1. What aspects of the design of fastai's callback system make it as flexible as copying and pasting bits of code?\n",
    "1. How can you get the list of events available to you when writing a callback?\n",
    "1. Write the `ModelResetter` callback (without peeking).\n",
    "1. How can you access the necessary attributes of the training loop inside a callback? When can you use or not use the shortcuts that go with them?\n",
    "1. How can a callback influence the control flow of the training loop.\n",
    "1. Write the `TerminateOnNaN` callback (without peeking, if possible).\n",
    "1. How do you make sure your callback runs after or before another callback?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
