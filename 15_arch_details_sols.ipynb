{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questionnaire"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__1. What is the \"head\" of a neural net?__\n",
    "\n",
    "The part of the net that is specialised for a particular task. Usually the part after the avg pooling layer (for a CNN)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__2. What is the \"body\" of a neural net?__\n",
    "\n",
    "The rest of the network. Includes the stem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__3. What is \"cutting\" a neural net? Why do we need to do this for transfer learning?__\n",
    "\n",
    "Cutting is the process of slicing a neural net. We want to remove certain layers from the net and replace them with customised layers. This process is necessary for transfer learning because it customises the network for our requirements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__4. What is `model_meta`? Try printing it to see what's inside.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A dict of information to determine where the body ends and the head starts. We need to replace the head with our customised version for transfer learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[i. for i in model_meta.keys()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "[<function fastai.vision.models.xresnet.xresnet18>,\n",
    " <function fastai.vision.models.xresnet.xresnet34>,\n",
    " <function fastai.vision.models.xresnet.xresnet50>,\n",
    " <function fastai.vision.models.xresnet.xresnet101>,\n",
    " <function fastai.vision.models.xresnet.xresnet152>,\n",
    " <function torchvision.models.resnet.resnet18>,\n",
    " <function torchvision.models.resnet.resnet34>,\n",
    " <function torchvision.models.resnet.resnet50>,\n",
    " <function torchvision.models.resnet.resnet101>,\n",
    " <function torchvision.models.resnet.resnet152>,\n",
    " <function torchvision.models.squeezenet.squeezenet1_0>,\n",
    " <function torchvision.models.squeezenet.squeezenet1_1>,\n",
    " <function torchvision.models.densenet.densenet121>,\n",
    " <function torchvision.models.densenet.densenet169>,\n",
    " <function torchvision.models.densenet.densenet201>,\n",
    " <function torchvision.models.densenet.densenet161>,\n",
    " <function torchvision.models.vgg.vgg11_bn>,\n",
    " <function torchvision.models.vgg.vgg13_bn>,\n",
    " <function torchvision.models.vgg.vgg16_bn>,\n",
    " <function torchvision.models.vgg.vgg19_bn>,\n",
    " <function torchvision.models.alexnet.alexnet>]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__5. Read the source code for `create_head` and make sure you understand what each line does.__\n",
    "\n",
    "Code as of 11/04/2021:\n",
    "\n",
    "```python\n",
    "if concat_pool: nf *= 2\n",
    "lin_ftrs = [nf, 512, n_out] if lin_ftrs is None else [nf] + lin_ftrs + [n_out]\n",
    "bns = [first_bn] + [True]*len(lin_ftrs[1:])\n",
    "ps = L(ps)\n",
    "if len(ps) == 1: ps = [ps[0]/2] * (len(lin_ftrs)-2) + ps\n",
    "actns = [nn.ReLU(inplace=True)] * (len(lin_ftrs)-2) + [None]\n",
    "pool = AdaptiveConcatPool2d() if concat_pool else nn.AdaptiveAvgPool2d(1)\n",
    "layers = [pool, Flatten()]\n",
    "if lin_first: layers.append(nn.Dropout(ps.pop(0)))\n",
    "for ni,no,bn,p,actn in zip(lin_ftrs[:-1], lin_ftrs[1:], bns, ps, actns):\n",
    "    layers += LinBnDrop(ni, no, bn=bn, p=p, act=actn, lin_first=lin_first)\n",
    "if lin_first: layers.append(nn.Linear(lin_ftrs[-2], n_out))\n",
    "if bn_final: layers.append(nn.BatchNorm1d(lin_ftrs[-1], momentum=0.01))\n",
    "if y_range is not None: layers.append(SigmoidRange(*y_range))\n",
    "return nn.Sequential(*layers)\n",
    "```\n",
    "\n",
    "Line by line:\n",
    "\n",
    "```python\n",
    "if concat_pool: nf *= 2\n",
    "```\n",
    "If `concat_pool` is activated then we need to use the concat-pool trick. This requires us to multiply the number of filters by 2 since we concat max pool and average pool.\n",
    "\n",
    "```python\n",
    "lin_ftrs = [nf, 512, n_out] if lin_ftrs is None else [nf] + lin_ftrs + [n_out]\n",
    "```\n",
    "Set size of layers.\n",
    "\n",
    "```python\n",
    "bns = [first_bn] + [True]*len(lin_ftrs[1:])\n",
    "```\n",
    "\n",
    "List of Batch Norms. True means that batch norm is applied.\n",
    "\n",
    "```python\n",
    "ps = L(ps)\n",
    "if len(ps) == 1: ps = [ps[0]/2] * (len(lin_ftrs)-2) + ps\n",
    "```\n",
    "List of Dropout probabilities. Dropout is divided by 2 for every layer except the last one. \n",
    "\n",
    "\n",
    "```python\n",
    "actns = [nn.ReLU(inplace=True)] * (len(lin_ftrs)-2) + [None]\n",
    "```\n",
    "\n",
    "List of activations. ReLU is provided for every layer except the last one.\n",
    "\n",
    "```python\n",
    "pool = AdaptiveConcatPool2d() if concat_pool else nn.AdaptiveAvgPool2d(1)\n",
    "```\n",
    "\n",
    "Add pooling layer. If concat_pool selected we use `AdaptiveConcatPool2d`. Else we use `AdaptiveAvgPool2d` with averages the activations into a map of dimension `1x1`.\n",
    "\n",
    "```python\n",
    "layers = [pool, Flatten()]\n",
    "```\n",
    "\n",
    "Add the pooling layer and the Flatten layer. Flatten removes the unit axes. \n",
    "\n",
    "```python\n",
    "if lin_first: layers.append(nn.Dropout(ps.pop(0)))\n",
    "```\n",
    "\n",
    "Add dropout layer to network\n",
    "\n",
    "\n",
    "```python\n",
    "for ni,no,bn,p,actn in zip(lin_ftrs[:-1], lin_ftrs[1:], bns, ps, actns):\n",
    "    layers += LinBnDrop(ni, no, bn=bn, p=p, act=actn, lin_first=lin_first)\n",
    "```\n",
    "\n",
    "Create the rest of the layers. Batch Norm (`bn`), Activations (`actn`), Dropout (`p`) is determined by lists made earlier. `LinBnDrop` is a specialised class that combines `BatchNorm1d`, `Dropout` and `Linear` layers.\n",
    "\n",
    "```python\n",
    "if lin_first: layers.append(nn.Linear(lin_ftrs[-2], n_out))\n",
    "```\n",
    "\n",
    "Add linear layer with appropriate sizes. This outputs to the number of required channels.\n",
    "\n",
    "```python\n",
    "if bn_final: layers.append(nn.BatchNorm1d(lin_ftrs[-1], momentum=0.01))\n",
    "```\n",
    "\n",
    "Add final batch norm layer with appropriate size.\n",
    "\n",
    "```python\n",
    "if y_range is not None: layers.append(SigmoidRange(*y_range))\n",
    "```\n",
    "\n",
    "Add final sigmoid layer. This will ensure that outputs are between the desired range.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__6. Figure out how to change the dropout, layer size, and number of layers created by `create_cnn`, and see if you can find values that result in better accuracy from the pet recognizer.__\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__7. What does `AdaptiveConcatPool2d` do?__\n",
    "\n",
    "It combines `AdaptiveAvgPool2d` and `AdaptiveMaxPool2d`\n",
    "\n",
    "```\n",
    "(0): AdaptiveConcatPool2d(\n",
    "    (ap): AdaptiveAvgPool2d(output_size=1)\n",
    "    (mp): AdaptiveMaxPool2d(output_size=1)\n",
    "  )\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__8. What is \"nearest neighbor interpolation\"? How can it be used to upsample convolutional activations?__\n",
    "\n",
    "It is a layer that increases the grid size of the activation map. Replace every pixel in the grid with four pixels in a 2x2 square."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__9. What is a \"transposed convolution\"? What is another name for it?__\n",
    "\n",
    "Zero padding is inserted between all pixels in input. This explains it pretty well:\n",
    "\n",
    "![img](https://raw.githubusercontent.com/fastai/fastbook/master/images/att_00051.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__10. Create a conv layer with `transpose=True` and apply it to an image. Check the output shape.__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://storage.googleapis.com/pr-newsroom-wp/1/2020/05/IMG_1874-copy-1.jpg # joe rogan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision, PIL\n",
    "x = torchvision.transforms.functional.to_tensor(PIL.Image.open('IMG_1874-copy-1.jpg'))\n",
    "x = x[None,:] # add unit axes (represents batch)\n",
    "x.shape\n",
    "# [81]: torch.Size([1, 3, 733, 1920])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_tranposed_conv(stride = 2):\n",
    "  tranposed_conv = ConvLayer(3, 3, stride=2, transpose=True) # n out channels is arbitrarily set to 3\n",
    "  return tranposed_conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = run_tranposed_conv(stride = 2)\n",
    "y.shape\n",
    "# [82]: torch.Size([1, 3, 1467, 3841])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = run_tranposed_conv(stride = 4)\n",
    "y.shape\n",
    "# [82]: torch.Size([1, 3, 2931, 7679])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dim of the activation map has increased by (approximately) by the stride. \n",
    "\n",
    "\n",
    "ie. (1467, 3841) approximately equal to (733*2, 1920*2)\n",
    "\n",
    "\n",
    "ie. (2931, 7679) approximately equal to (733*4, 1920*4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__11. Draw the U-Net architecture.__\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__12. What is \"BPTT for Text Classification\" (BPT3C)?__\n",
    "\n",
    "It's a classifier which consists of a for loop that loops over each batch of a sequence of text. The activations of each batch are stored. Then at the end a pool_concat is used over the RNN sequence to get the desired output grid size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__13. How do we handle different length sequences in BPT3C?__\n",
    "\n",
    "Padding. \n",
    "\n",
    "We determine the text with the longest length. Then we use the special token `xxpad` to fill up all lines that have length < longest length."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__14. Try to run each line of `TabularModel.forward` separately, one line per cell, in a notebook, and look at the input and output shapes at each step.__\n",
    "\n",
    "Forward function\n",
    "\n",
    "```python\n",
    "def forward(self, x_cat, x_cont=None):\n",
    "    if self.n_emb != 0:\n",
    "        x = [e(x_cat[:,i]) for i,e in enumerate(self.embeds)]\n",
    "        x = torch.cat(x, 1)\n",
    "        x = self.emb_drop(x)\n",
    "    if self.n_cont != 0:\n",
    "        if self.bn_cont is not None: x_cont = self.bn_cont(x_cont)\n",
    "        x = torch.cat([x, x_cont], 1) if self.n_emb != 0 else x_cont\n",
    "    return self.layers(x)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.tabular.model import TabularModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TabularModel(emb_szs = [[x.shape[0],x.shape[-1]]], n_cont = 0, out_sz = 10, layers = [500])\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```TabularModel(\n",
    "  (embeds): ModuleList(\n",
    "    (0): Embedding(3, 1920)\n",
    "  )\n",
    "  (emb_drop): Dropout(p=0.0, inplace=False)\n",
    "  (bn_cont): BatchNorm1d(0, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "  (layers): Sequential(\n",
    "    (0): LinBnDrop(\n",
    "      (0): BatchNorm1d(1920, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "      (1): Linear(in_features=1920, out_features=500, bias=False)\n",
    "      (2): ReLU(inplace=True)\n",
    "    )\n",
    "    (1): LinBnDrop(\n",
    "      (0): Linear(in_features=500, out_features=10, bias=True)\n",
    "    )\n",
    "  )\n",
    ")```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "tensor([[[ 2.1888e-01, -2.8956e-02,  1.6438e-01,  ...,  1.9798e-04,  2.4943e-01, -2.9432e-01],\n",
    "         [ 2.1888e-01, -2.8956e-02,  1.6438e-01,  ...,  1.9798e-04,  2.4943e-01, -2.9432e-01],\n",
    "         [ 2.1888e-01, -2.8956e-02,  1.6438e-01,  ...,  1.9798e-04,  2.4943e-01, -2.9432e-01],\n",
    "         ...,\n",
    "         [ 2.1888e-01, -2.8956e-02,  1.6438e-01,  ...,  1.9798e-04,  2.4943e-01, -2.9432e-01],\n",
    "         [ 2.1888e-01, -2.8956e-02,  1.6438e-01,  ...,  1.9798e-04,  2.4943e-01, -2.9432e-01],\n",
    "         [ 2.1888e-01, -2.8956e-02,  1.6438e-01,  ...,  1.9798e-04,  2.4943e-01, -2.9432e-01]],\n",
    "\n",
    "        [[ 2.1888e-01, -2.8956e-02,  1.6438e-01,  ...,  1.9798e-04,  2.4943e-01, -2.9432e-01],\n",
    "         [ 2.1888e-01, -2.8956e-02,  1.6438e-01,  ...,  1.9798e-04,  2.4943e-01, -2.9432e-01],\n",
    "         [ 2.1888e-01, -2.8956e-02,  1.6438e-01,  ...,  1.9798e-04,  2.4943e-01, -2.9432e-01],\n",
    "         ...,\n",
    "         [ 2.1888e-01, -2.8956e-02,  1.6438e-01,  ...,  1.9798e-04,  2.4943e-01, -2.9432e-01],\n",
    "         [ 2.1888e-01, -2.8956e-02,  1.6438e-01,  ...,  1.9798e-04,  2.4943e-01, -2.9432e-01],\n",
    "         [ 2.1888e-01, -2.8956e-02,  1.6438e-01,  ...,  1.9798e-04,  2.4943e-01, -2.9432e-01]],\n",
    "\n",
    "        [[ 2.1888e-01, -2.8956e-02,  1.6438e-01,  ...,  1.9798e-04,  2.4943e-01, -2.9432e-01],\n",
    "         [ 2.1888e-01, -2.8956e-02,  1.6438e-01,  ...,  1.9798e-04,  2.4943e-01, -2.9432e-01],\n",
    "         [ 2.1888e-01, -2.8956e-02,  1.6438e-01,  ...,  1.9798e-04,  2.4943e-01, -2.9432e-01],\n",
    "         ...,\n",
    "         [ 2.1888e-01, -2.8956e-02,  1.6438e-01,  ...,  1.9798e-04,  2.4943e-01, -2.9432e-01],\n",
    "         [ 2.1888e-01, -2.8956e-02,  1.6438e-01,  ...,  1.9798e-04,  2.4943e-01, -2.9432e-01],\n",
    "         [ 2.1888e-01, -2.8956e-02,  1.6438e-01,  ...,  1.9798e-04,  2.4943e-01, -2.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if self.n_emb != 0:\n",
    "model.n_emb!=0\n",
    "# [208]: True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = [e(x[:,i]) for i,e in enumerate(model.embeds)]\n",
    "y[0].shape\n",
    "# [208]: torch.Size([3, 1920, 1920])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.cat(y, 1)\n",
    "y.shape\n",
    "# [209]: torch.Size([3, 1920, 1920])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = model.emb_drop(y)\n",
    "y.shape\n",
    "# [210]: torch.Size([3, 1920, 1920])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if self.n_cont != 0:\n",
    "model.n_cont\n",
    "# [211]: 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if self.bn_cont is not None: \n",
    "model.bn_cont\n",
    "# [212]: BatchNorm1d(0, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_cont = self.bn_cont(x_cont)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = torch.cat([x, x_cont], 1) if self.n_emb != 0 else x_cont"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return self.layers(x)\n",
    "model.layers(y).shape\n",
    "# [215]: torch.Size([3, 1920, 10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__15. How is `self.layers` defined in `TabularModel`?__\n",
    "\n",
    "Relevant lines of code:\n",
    "\n",
    "```python\n",
    "_layers = [LinBnDrop(sizes[i], sizes[i+1], bn=use_bn and (i!=len(actns)-1 or bn_final), p=p, act=a)\n",
    "                       for i,(p,a) in enumerate(zip(ps+[0.],actns))]\n",
    "self.layers = nn.Sequential(*_layers)\n",
    "\n",
    "```\n",
    "\n",
    "This creates layers using the `LinBnDrop` function. The activations are defined by the list `actns`. The dropout prob is defined by `ps`. \n",
    "\n",
    "__Breakdown__\n",
    "\n",
    "The following determines if the layer should use Batch Norm:\n",
    "\n",
    "```python\n",
    "use_bn and (i!=len(actns)-1 or bn_final)\n",
    "```\n",
    "\n",
    "Cases where Batch Norm is applied:\n",
    "1. `use_bn` is True and `bn_final` is True => Batch Norm applied at that layer\n",
    "2. `use_bn` is True and `i!=len(actns)-1` (ie. we are not at the last layer) => Batch Norm applied\n",
    "\n",
    "In all other cases Batch Norm is not applied.\n",
    "\n",
    "\n",
    "The rest of the code is fairly straightforward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__16. What are the five steps for preventing over-fitting?__\n",
    "\n",
    "1. More data\n",
    "2. Augmentation\n",
    "3. Generalisable architectures (eg. BatchNorm)\n",
    "4. Regularization (eg. Dropout)\n",
    "5. Reduce architecture complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__17. Why don't we reduce architecture complexity before trying other approaches to preventing overfitting?__\n",
    "\n",
    "More parameters allows your model to learn about more subtle relationships in the data. You would miss these relationships if you chose a smaller model. This is why a larger model is recommended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
