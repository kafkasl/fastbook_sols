{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questionnaire"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__1. What is the equation for a step of SGD, in math or code (as you prefer)?__\n",
    "\n",
    "\n",
    "`p.data.add_(-lr, p.grad.data)`\n",
    "\n",
    "params = params - lr * params.gradient\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__2. What do we pass to `cnn_learner` to use a non-default optimizer?__\n",
    "\n",
    "You pass the following:\n",
    "`opt_func=partial(Optimizer, cbs=[sgd_cb])`\n",
    "\n",
    "where `def sgd_cb(p, lr, **kwargs): p.data.add_(-lr, p.grad.data)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__3. What are optimizer callbacks?__\n",
    "\n",
    "This: `opt_func = partial(Optimizer, cbs=[sgd_cb])`\n",
    "\n",
    "They help us run a desired optimiser (eg. sgd) using a few lines of code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__4. What does `zero_grad` do in an optimizer?__\n",
    "\n",
    "- Loops through params and sets gradients to 0\n",
    "- Removes gradient history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__5. What does `step` do in an optimizer? How is it implemented in the general optimizer?__\n",
    "\n",
    "It loops through the params and through all the callbacks. It uses the callbacks to update each param."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__6. Rewrite `sgd_cb` to use the `+=` operator, instead of `add_`.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd_cb(p, lr, **kwargs): p.data += -lr*p.grad.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__7. What is \"momentum\"? Write out the equation.__\n",
    "\n",
    "Exponentially weighted moving average. This equation is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_avg = beta * weight_avg + (1-beta) * p.grad.data\n",
    "p.data = p.data - lr * weight_avg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__8. What's a physical analogy for momentum? How does it apply in our model training settings?__\n",
    "\n",
    "Above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__9. What does a bigger value for momentum do to the gradients?__\n",
    "\n",
    "The gradients will be closer toward the average gradient. It will take a while before the gradients make the trend move."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__10. What are the default values of momentum for 1cycle training?__\n",
    "\n",
    "fit_one_cycle by default starts with a beta of 0.95, gradually adjusts it to 0.85, and then gradually moves it back to 0.95 at the end of training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__11. What is RMSProp? Write out the equation.__\n",
    "\n",
    "It takes the moving average in a different way in order to reduce the noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w.square_avg = alpha * w.square_avg + (1-alpha) * (w.grad ** 2)\n",
    "new_w = w - lr * w.grad / math.sqrt(w.square_avg + eps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__12. What do the squared values of the gradients indicate?__\n",
    "\n",
    "It indicates they've been squared?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__13. How does Adam differ from momentum and RMSProp?__\n",
    "\n",
    "Momentum takes the exponentially weighted moving average.\n",
    "\n",
    "RMSProp takes the moving average in a different way that involves taking the square and then the sqrt root.\n",
    "\n",
    "Adam mixes the two ideas together. It uses the moving average and divides by the square root of the moving average to give an adaptive learning rate for each param."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__14. Write out the equation for Adam.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta1,beta2 = 0.9,0.999\n",
    "w.avg = beta1 * w.avg + (1-beta1) * w.grad\n",
    "unbias_avg = w.avg / (1 - (beta1**(i+1)))\n",
    "w.sqr_avg = beta2 * w.sqr_avg + (1-beta2) * (w.grad ** 2)\n",
    "new_w = w - lr * unbias_avg / sqrt(w.sqr_avg + eps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__15. Calculate the values of `unbias_avg` and `w.avg` for a few batches of dummy values.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: check if this is right\n",
    "\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "\n",
    "beta1,beta2 = 0.9,0.999\n",
    "lr = 1e-3\n",
    "eps = 1e-8\n",
    "w_avg = 0.\n",
    "w_sqr_avg = 0.\n",
    "\n",
    "# Calculate dummy gradients\n",
    "model = models.resnet34()\n",
    "model(torch.randn(1, 3, 224, 224)).mean().backward() # bs x 3 channels x height x width\n",
    "\n",
    "for i, params in enumerate(model.parameters()):\n",
    "  print(params.grad.data.shape)\n",
    "  w_avg = beta1 * w_avg + (1-beta1) * params.grad.data\n",
    "  unbias_avg = w_avg / (1 - (beta1**(i+1)))\n",
    "  w_sqr_avg = beta2 * w_sqr_avg + (1-beta2) * (params.grad.data ** 2)\n",
    "  params.data = params.data - lr * unbias_avg / torch.sqrt(w_sqr_avg + eps)\n",
    "  print(unbias_avg.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__16. What's the impact of having a high `eps` in Adam?__\n",
    "\n",
    "As `eps` becomes larger `w.grad / math.sqrt(w.square_avg + eps)` becomes smaller.\n",
    "\n",
    "Therefore, `lr * w.grad / math.sqrt(w.square_avg + eps)` becomes smaller.\n",
    "\n",
    "So, `new_w` stays closer to `w`.\n",
    "\n",
    "Since, `new_w = w - lr * w.grad / math.sqrt(w.square_avg + eps)`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__17. Read through the optimizer notebook in fastai's repo, and execute it.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__18. In what situations do dynamic learning rate methods like Adam change the behavior of weight decay?__\n",
    "\n",
    "When momentum is added.\n",
    "\n",
    "ie when `beta1` is non-zero in the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w.avg = beta1 * w.avg + (1-beta1) * w.grad\n",
    "unbias_avg = w.avg / (1 - (beta1**(i+1)))\n",
    "w.sqr_avg = beta2 * w.sqr_avg + (1-beta2) * (w.grad ** 2)\n",
    "new_w = w - lr * unbias_avg / sqrt(w.sqr_avg + eps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__19. What are the four steps of a training loop?__\n",
    "\n",
    "```python\n",
    "loss = loss_func(model(xb), yb)\n",
    "loss.backward()\n",
    "opt.step()\n",
    "opt.zero_grad()\n",
    "```\n",
    "\n",
    "This is done for every `xb` and `yb` in the batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__20. Why is using callbacks better than writing a new training loop for each tweak you want to add?__\n",
    "\n",
    "Consistent and well defined\n",
    "\n",
    "Modularity\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__21. What aspects of the design of fastai's callback system make it as flexible as copying and pasting bits of code?__\n",
    "\n",
    "The callback can read every piece of information availiale in the training loop. Fastai provides this functionality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__22. How can you get the list of events available to you when writing a callback?__\n",
    "\n",
    "Type `event.` and hit Tab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__23. Write the `ModelResetter` callback (without peeking).__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelResetter(Callback):\n",
    "    def begin_train(self): self.model.reset()\n",
    "    def begin_validate(self): self.model.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__24. How can you access the necessary attributes of the training loop inside a callback? When can you use or not use the shortcuts that go with them?__\n",
    "\n",
    "You can create specialised functions in the callback itself. The names of these functions should match the print out from the events variable.\n",
    "\n",
    "You can access attributes associated with the learner such as model and data. The full list in mentioned in the chapter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__25. Write the `TerminateOnNaN` callback (without peeking, if possible).__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TerminateOnNaN(Callback):\n",
    "    run_before = Recorder\n",
    "    def after_batch(self):\n",
    "        if torch.isinf(self.loss) or torch.isnan(self.loss):\n",
    "            raise CancelFitException"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__26. How do you make sure your callback runs after or before another callback?__\n",
    "\n",
    "You can use the `run_before` or `run_after` parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
