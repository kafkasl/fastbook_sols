{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questionnaire"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Write the Python code to implement a single neuron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neuron_output = sum([x*w for x,w in zip(inputs,weights)]) + bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__2. Write the Python code to implement ReLU.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x): return 0 if x<0 else x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__3. Write the Python code for a dense layer in terms of matrix multiplication.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = x @ w.t() + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__4. Write the Python code for a dense layer in plain Python (that is, with list comprehensions and functionality built into Python).__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y[i,j] = sum([a * b for a,b in zip(x[i,:],w[j,:])]) + b[j]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__5. What is the \"hidden size\" of a layer?__\n",
    "\n",
    "The number of neurons in that layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__6. What does the `t` method do in PyTorch?__\n",
    "\n",
    "Tranposes the matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__7. Why is matrix multiplication written in plain Python very slow?__\n",
    "\n",
    "It's too high level. C++ is used in Pytorch to speed things up."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__8. In `matmul`, why is `ac==br`?__\n",
    "\n",
    "The number of columns in matrix `a` must equal the number of rows in matrix `b`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__9. In Jupyter Notebook, how do you measure the time taken for a single cell to execute?__\n",
    "\n",
    "`%time`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__10. What is \"elementwise arithmetic\"?__\n",
    "\n",
    "When a mathematical operation is applied to every cell in the tensor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__11. Write the PyTorch code to test whether every element of `a` is greater than the corresponding element of `b`.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(a < b).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__12. What is a rank-0 tensor? How do you convert it to a plain Python data type?__\n",
    "\n",
    "A rank-0 tensor is a tensor that returns one element\n",
    "\n",
    "You can turn it into a plain Python data type using the `.item()` command"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__13. What does this return, and why? `tensor([1,2]) + tensor([1])`__\n",
    "\n",
    "Should given an error. The tensors don't have the same shapes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__14. What does this return, and why? `tensor([1,2]) + tensor([1,2,3])`__\n",
    "\n",
    "Same as above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__15. How does elementwise arithmetic help us speed up `matmul`?__\n",
    "\n",
    "We can do this: `a[i,:] * b[:,j]`. This takes the product of one row from `a` and one column at `b` at each cell. Then we call `.sum()` on the result. This removes an entire loop from the `matmul` function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__16. What are the broadcasting rules?__\n",
    "\n",
    "Pytorch compares tensor shapes elementwise. It starts with the ending dimensions and works it way backward. It will add 1 where it sees empty dimensions. Stopping criteria:\n",
    "\n",
    "- Tensors are equal\n",
    "- One of the tensors is 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__17. What is `expand_as`? Show an example of how it can be used to match the results of broadcasting.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = tensor([10.,20,30])\n",
    "m = tensor([[1., 2, 3], [4,5,6], [7,8,9]])\n",
    "c.expand_as(m)\n",
    "# [22]: tensor([[10., 20., 30.],\n",
    "#         [10., 20., 30.],\n",
    "#         [10., 20., 30.]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__18. How does `unsqueeze` help us to solve certain broadcasting problems?__\n",
    "\n",
    "It adds a unit dimension to our tensor. This helps if we need to broadcast tensors of different shapes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__19. How can we use indexing to do the same operation as `unsqueeze`?__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the following explains it\n",
    "\n",
    "c = torch.randn(64,28,28)\n",
    "c.shape\n",
    "# torch.Size([64, 28, 28])\n",
    "\n",
    "c.unsqueeze(1).shape\n",
    "# torch.Size([64, 1, 28, 28])\n",
    "\n",
    "c[:,None].shape\n",
    "# torch.Size([64, 1, 28, 28])\n",
    "\n",
    "c[:,:, None].shape\n",
    "# torch.Size([64, 28, 1, 28])\n",
    "\n",
    "c[...,None].shape\n",
    "# torch.Size([64, 28, 28, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__20. How do we show the actual contents of the memory used for a tensor?__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.storage()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__21. When adding a vector of size 3 to a matrix of size 3√ó3, are the elements of the vector added to each row or each column of the matrix? (Be sure to check your answer by running this code in a notebook.)__\n",
    "\n",
    "The elements of the vector are added to each row of the matrix as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = torch.randn(3)\n",
    "d = torch.randn(3,3)\n",
    "c.shape, d.shape\n",
    "# (torch.Size([3]), torch.Size([3, 3]))\n",
    "\n",
    "c\n",
    "# tensor([ 0.2224, -0.3962,  2.0097])\n",
    "\n",
    "d\n",
    "# tensor([[-0.2752, -0.3189,  0.5216],\n",
    "#         [ 0.6154, -0.1518, -1.0665],\n",
    "#         [ 0.0169, -0.0300,  1.0752]])\n",
    "\n",
    "c+d\n",
    "# tensor([[-0.0528, -0.7152,  2.5313],\n",
    "#         [ 0.8378, -0.5480,  0.9433],\n",
    "#         [ 0.2393, -0.4262,  3.0849]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__22. Do broadcasting and `expand_as` result in increased memory use? Why or why not?__\n",
    "\n",
    "No it does not. Pytorch gives the tensor a stride of 0. So when it looks for the next row by adding the stride - it doesn't move."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__23. Implement `matmul` using Einstein summation.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matmul(a,b): return torch.einsum('ik,kj->ij', a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__24. What does a repeated index letter represent on the left-hand side of einsum?__\n",
    "\n",
    "The number of rows in one matrix need to equal the number of cols in another matrix. Eg. in the question above the letter `k` is repeated on the LHS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__25. What are the three rules of Einstein summation notation? Why?__\n",
    "\n",
    "Repeated indices are implicitly summed over. Above, the term k is repeated so we sum over that index.\n",
    "\n",
    "Each index can appear at most twice in any term. \n",
    "\n",
    "Each term must contain identical nonrepeated indices.\n",
    "\n",
    "\n",
    "TODO: fill in why"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__26. What are the forward pass and backward pass of a neural network?__\n",
    "\n",
    "forward pass: compute the output of a model given input and weights\n",
    "\n",
    "backward pass: compute gradients of each layer starting at the end of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__27. Why do we need to store some of the activations calculated for intermediate layers in the forward pass?__\n",
    "\n",
    "So we can calculate the gradients in the backward pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__28. What is the downside of having activations with a standard deviation too far away from 1?__\n",
    "\n",
    "We run the risk of the activations becoming `nans` as they are too large"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__29. How can weight initialization help avoid this problem?__\n",
    "\n",
    "We can mulitply each layer by a scale value: `1/‚àöùëõùëñùëõ`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__30. What is the formula to initialize weights such that we get a standard deviation of 1 for a plain linear layer, and for a linear layer followed by ReLU?__\n",
    "\n",
    "Multiply each layer by the scale value: `2/‚àöùëõùëñùëõ`. Kaiming init."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__31. Why do we sometimes have to use the `squeeze` method in loss functions?__\n",
    "\n",
    "To get rid of the trailing unit dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__32. What does the argument to the `squeeze` method do? Why might it be important to include this argument, even though PyTorch does not require it?__\n",
    "\n",
    "The argument tells PyTorch what axis to remove the unit dimensions from. It can be sometimes useful to hardcode this yourself for clarity?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__33. What is the \"chain rule\"? Show the equation in either of the two forms presented in this chapter.__\n",
    "\n",
    "$$\\frac{\\text{d} loss}{\\text{d} b_{2}} = \\frac{\\text{d} loss}{\\text{d} out} \\times \\frac{\\text{d} out}{\\text{d} b_{2}} = \\frac{\\text{d}}{\\text{d} out} mse(out, y) \\times \\frac{\\text{d}}{\\text{d} b_{2}} lin(l_{2}, w_{2}, b_{2})$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__34. Show how to calculate the gradients of `mse(lin(l2, w2, b2), y)` using the chain rule.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lin(x, w, b): return x @ w + b\n",
    "def mse(output, targ): return (output.squeeze(-1) - targ).pow(2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1. What is the gradient of ReLU? Show it in math or code. (You shouldn't need to commit this to memory‚Äîtry to figure it using your knowledge of the shape of the function.)\n",
    "1. In what order do we need to call the `*_grad` functions in the backward pass? Why?\n",
    "1. What is `__call__`?\n",
    "1. What methods must we implement when writing a `torch.autograd.Function`?\n",
    "1. Write `nn.Linear` from scratch, and test it works.\n",
    "1. What is the difference between `nn.Module` and fastai's `Module`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
